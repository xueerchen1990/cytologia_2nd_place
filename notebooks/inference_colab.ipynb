{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1736709814534,"user":{"displayName":"xueer","userId":"12210123935039791816"},"user_tz":300},"id":"V-UQE1_ydlCX","outputId":"1547cde2-2703-4581-b374-2890e9ff5c9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jan 12 19:23:34 2025       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":4898,"status":"ok","timestamp":1736709820059,"user":{"displayName":"xueer","userId":"12210123935039791816"},"user_tz":300},"id":"vVopogb1NQgk","outputId":"027adce9-278f-4224-f372-81c79f5144a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics==8.3.49\n","  Downloading ultralytics-8.3.49-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.3.49) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics==8.3.49)\n","  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.49) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.3.49) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.3.49) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.3.49) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.3.49) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.3.49) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.3.49) (2024.12.14)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.3.49) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics==8.3.49) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.49) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.49) (3.0.2)\n","Downloading ultralytics-8.3.49-py3-none-any.whl (898 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.49 ultralytics-thop-2.0.13\n"]}],"source":["!pip install ultralytics==8.3.49"]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/cytologia'"],"metadata":{"id":"qgB1L97P7mHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23926,"status":"ok","timestamp":1736709843983,"user":{"displayName":"xueer","userId":"12210123935039791816"},"user_tz":300},"id":"SsgpkF8SMrHz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18705e02-d358-4a3b-a851-c7f6bcfa7cd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import torch\n","import torchvision\n","import numpy as np\n","from ultralytics.utils import LOGGER\n","from tqdm import tqdm\n","import os\n","from PIL import Image\n","import time\n","from ultralytics import YOLO\n","from ultralytics.models.yolo.detect import DetectionPredictor\n","from ultralytics.engine.results import Results\n","from ultralytics.utils import ops\n","from random import random\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import sys\n","sys.path.append(f'{data_path}/scripts')\n","from metrics import compute_cytologia_metric_optimized"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cy4ddgYcdCy"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import pandas as pd\n","def load_images(image_files):\n","    \"\"\"\n","    Reads all JPG images returns them as a list of NumPy array in RGB format.\n","    \"\"\"\n","\n","    images = []\n","\n","    for img_file in tqdm(image_files):\n","        img = cv2.imread(img_file)  # Read image in BGR format\n","        if img is not None:\n","            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n","            images.append(img_rgb)\n","        else:\n","            print(f\"Warning: Could not read image '{img_file}'. Skipping.\")\n","    return images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfpUhcvicnhB"},"outputs":[],"source":["\n","test = pd.read_csv(os.path.join(data_path, 'test.csv'))\n","img_folder = os.path.join(data_path, 'images')\n","test_imgs = test['NAME'].apply(lambda x: img_folder+'/'+x).values.tolist()\n","# test_imgs_array = load_images(test_imgs)\n","# np.savez_compressed('test_images.npz', *test_imgs_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2256,"status":"ok","timestamp":1736709847457,"user":{"displayName":"xueer","userId":"12210123935039791816"},"user_tz":300},"id":"asTTtzWGSJGG","outputId":"fdc67787-98bd-4407-d608-6fc7cf3e628a"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 259 ms, sys: 10.8 ms, total: 270 ms\n","Wall time: 2.26 s\n"]}],"source":["%%time\n","test_imgs_array = np.load(os.path.join(data_path,'test_images.npz'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1736709848627,"user":{"displayName":"xueer","userId":"12210123935039791816"},"user_tz":300},"id":"83_KQK3kg-zY","outputId":"942f42a1-0603-4c24-ed25-3877bb31efc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["arr_20746\n","arr_20747\n","arr_20748\n","arr_20749\n","arr_20750\n"]}],"source":["for arr_name in test_imgs_array.files[-5:]:\n","    print(arr_name)\n","test_imgs_array[arr_name].shape\n","for arr_name in test_imgs_array.files[-5:]:\n","    img = Image.fromarray(test_imgs_array[arr_name])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_V-Bgmoh5r9"},"outputs":[],"source":["def my_non_max_suppression(\n","    prediction,\n","    conf_thres=0.25,\n","    iou_thres=0.45,\n","    classes=None,\n","    agnostic=False,\n","    multi_label=False,\n","    labels=(),\n","    max_det=300,\n","    nc=0,  # number of classes (optional)\n","    max_time_img=0.05,\n","    max_nms=30000,\n","    max_wh=7680,\n","    in_place=True,\n","    rotated=False,\n","):\n","    \"\"\"\n","    Perform non-maximum suppression (NMS) on a set of boxes, with support for masks and multiple labels per box.\n","\n","    Args:\n","        prediction (torch.Tensor): A tensor of shape (batch_size, num_classes + 4 + num_masks, num_boxes)\n","            containing the predicted boxes, classes, and masks. The tensor should be in the format\n","            output by a model, such as YOLO.\n","        conf_thres (float): The confidence threshold below which boxes will be filtered out.\n","            Valid values are between 0.0 and 1.0.\n","        iou_thres (float): The IoU threshold below which boxes will be filtered out during NMS.\n","            Valid values are between 0.0 and 1.0.\n","        classes (List[int]): A list of class indices to consider. If None, all classes will be considered.\n","        agnostic (bool): If True, the model is agnostic to the number of classes, and all\n","            classes will be considered as one.\n","        multi_label (bool): If True, each box may have multiple labels.\n","        labels (List[List[Union[int, float, torch.Tensor]]]): A list of lists, where each inner\n","            list contains the apriori labels for a given image. The list should be in the format\n","            output by a dataloader, with each label being a tuple of (class_index, x1, y1, x2, y2).\n","        max_det (int): The maximum number of boxes to keep after NMS.\n","        nc (int, optional): The number of classes output by the model. Any indices after this will be considered masks.\n","        max_time_img (float): The maximum time (seconds) for processing one image.\n","        max_nms (int): The maximum number of boxes into torchvision.ops.nms().\n","        max_wh (int): The maximum box width and height in pixels.\n","        in_place (bool): If True, the input prediction tensor will be modified in place.\n","        rotated (bool): If Oriented Bounding Boxes (OBB) are being passed for NMS.\n","\n","    Returns:\n","        (List[torch.Tensor]): A list of length batch_size, where each element is a tensor of\n","            shape (num_boxes, 6 + num_masks + num_classes) containing the kept boxes, with columns\n","            (x1, y1, x2, y2, confidence, class, mask1, mask2, ..., all class probabilities).\n","    \"\"\"\n","\n","    # Checks\n","    assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n","    assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n","    if isinstance(prediction, (list, tuple)):  # YOLOv8 model in validation model, output = (inference_out, loss_out)\n","        prediction = prediction[0]  # select only inference output\n","    if classes is not None:\n","        classes = torch.tensor(classes, device=prediction.device)\n","\n","    if prediction.shape[-1] == 6:  # end-to-end model (BNC, i.e. 1,300,6)\n","        output = [pred[pred[:, 4] > conf_thres][:max_det] for pred in prediction]\n","        if classes is not None:\n","            output = [pred[(pred[:, 5:6] == classes).any(1)] for pred in output]\n","        return output\n","\n","    bs = prediction.shape[0]  # batch size (BCN, i.e. 1,84,6300)\n","    nc = nc or (prediction.shape[1] - 4)  # number of classes\n","    nm = prediction.shape[1] - nc - 4  # number of masks\n","    mi = 4 + nc  # mask start index\n","    xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n","\n","    # Settings\n","    # min_wh = 2  # (pixels) minimum box width and height\n","    time_limit = 2.0 + max_time_img * bs  # seconds to quit after\n","    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n","\n","    prediction = prediction.transpose(-1, -2)  # shape(1,84,6300) to shape(1,6300,84)\n","    if not rotated:\n","        if in_place:\n","            prediction[..., :4] = ops.xywh2xyxy(prediction[..., :4])  # xywh to xyxy\n","        else:\n","            prediction = torch.cat((ops.xywh2xyxy(prediction[..., :4]), prediction[..., 4:]), dim=-1)  # xywh to xyxy\n","\n","    t = time.time()\n","    output = [torch.zeros((0, 6 + nm + nc), device=prediction.device)] * bs\n","    for xi, x in enumerate(prediction):  # image index, image inference\n","        # Apply constraints\n","        # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n","        x = x[xc[xi]]  # confidence\n","\n","        # Cat apriori labels if autolabelling\n","        if labels and len(labels[xi]) and not rotated:\n","            lb = labels[xi]\n","            v = torch.zeros((len(lb), nc + nm + 4), device=x.device)\n","            v[:, :4] = ops.xywh2xyxy(lb[:, 1:5])  # box\n","            v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n","            x = torch.cat((x, v), 0)\n","\n","        # If none remain process next image\n","        if not x.shape[0]:\n","            continue\n","\n","        # Detections matrix nx6 (xyxy, conf, cls)\n","        box, cls, mask = x.split((4, nc, nm), 1)\n","        # Store all class probabilities\n","        all_cls_probs = cls\n","\n","        if multi_label:\n","            i, j = torch.where(cls > conf_thres)\n","            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n","        else:  # best class only\n","            conf, j = cls.max(1, keepdim=True)\n","            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n","\n","        # Filter by class\n","        if classes is not None:\n","            x = x[(x[:, 5:6] == classes).any(1)]\n","\n","        # Check shape\n","        n = x.shape[0]  # number of boxes\n","        if not n:  # no boxes\n","            continue\n","        if n > max_nms:  # excess boxes\n","            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n","\n","        # Batched NMS\n","        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n","        scores = x[:, 4]  # scores\n","        if rotated:\n","            boxes = torch.cat((x[:, :2] + c, x[:, 2:4], x[:, -1:]), dim=-1)  # xywhr\n","            i = ops.nms_rotated(boxes, scores, iou_thres)\n","        else:\n","            boxes = x[:, :4] + c  # boxes (offset by class)\n","            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n","        i = i[:max_det]  # limit detections\n","\n","        output[xi] = torch.cat((x[i], all_cls_probs[i]), dim=1)\n","\n","        if (time.time() - t) > time_limit:\n","            LOGGER.warning(f\"WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded\")\n","            break  # time limit exceeded\n","\n","    return output\n","\n","class MyCustomPredictor(DetectionPredictor):\n","\n","    def postprocess(self, preds, img, orig_imgs):\n","        \"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\n","        preds = my_non_max_suppression(\n","            preds,\n","            self.args.conf,\n","            self.args.iou,\n","            agnostic=self.args.agnostic_nms,\n","            max_det=self.args.max_det,\n","            classes=self.args.classes,\n","        )\n","\n","        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n","            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n","\n","        results = []\n","        probs = []\n","        for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0]):\n","            pred,prob = pred[:,:-23], pred[:,-23:]\n","            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\n","            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\n","            if len(prob):\n","                probs.append(prob.cpu().numpy()[0])\n","        self.probs.extend(probs)\n","        return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d4gy6xUycUv"},"outputs":[],"source":["def predict_det(df, model, aug, images_array, batch_size=1, max_det=11):\n","\n","    # Get model output names for later use\n","    img = Image.fromarray(images_array['arr_0'])\n","    res = model(img, verbose=False, augment=aug)\n","    names = res[0].names\n","\n","    my_predictor = MyCustomPredictor()\n","    my_predictor.args = model.predictor.args\n","    my_predictor.probs = []\n","    model.predictor = my_predictor\n","\n","    def dec(x):\n","        return [float(f'{i:.3f}') for i in x]\n","\n","    outs = []\n","    test_probs = []\n","    # Use tqdm with total number of images\n","    with tqdm(total=df.shape[0], desc=\"Processing images\") as pbar:\n","        for i in range(0, df.shape[0], batch_size):\n","            batch_rows = df.iloc[i: i + batch_size]\n","            imgs = [Image.fromarray(images_array[i]) for i in batch_rows['imgid']]\n","            results = model(imgs, max_det=max_det, verbose=False, augment=aug)\n","\n","            for j, (_, row) in enumerate(batch_rows.iterrows()):\n","                result = results[j]\n","                xyxys = result.boxes.xyxy.cpu().numpy().tolist()\n","                clss = result.boxes.cls.cpu().numpy().tolist()\n","                ids = tids[row['NAME']]\n","                probs = my_predictor.probs[j]\n","                for c, idx in enumerate(ids):\n","                    if c < len(clss):\n","                        x1, y1, x2, y2 = dec(xyxys[c])\n","                        cls_id = int(clss[c])\n","                        cls = names[cls_id]\n","                        outs.append([idx, row['NAME'], x1, y1, x2, y2, cls])\n","                    elif max_det > 1:\n","                        # copy last prediction but avoid exact duplication\n","                        outs.append([idx, row['NAME'], x1+random()*1e-5, y1, x2, y2, cls])\n","                    if batch_size == 1:\n","                        if len(outs) == len(my_predictor.probs)+1:\n","                            tmp = np.zeros(23)\n","                            tmp[cls_id] = 1\n","                            my_predictor.probs.append(tmp)\n","                        elif len(outs) != len(my_predictor.probs):\n","                            assert 0\n","                pbar.update(1)  # Update progress bar for each image\n","            assert len(outs) == len(my_predictor.probs)\n","\n","    sub = pd.DataFrame(outs, columns=['trustii_id', 'NAME', 'x1', 'y1', 'x2', 'y2', 'class'])\n","    probs = np.array(my_predictor.probs)\n","    return sub, probs, names\n","\n","def predict_cls(test, model, image_arrays, batch_size=32):\n","    \"\"\"\n","    Predicts the class of images in a DataFrame using a given model with batched inference.\n","\n","    Args:\n","        test: DataFrame containing image names, bounding box coordinates, and image IDs.\n","        model: The model used for prediction.\n","        image_arrays: Dictionary or list containing the image data.\n","        batch_size: The batch size for inference.\n","\n","    Returns:\n","        DataFrame with predicted class and probabilities for each class.\n","    \"\"\"\n","    cls_res = []\n","    probs = []\n","    for i in tqdm(range(0, test.shape[0], batch_size)):\n","        batch = test.iloc[i:i + batch_size]\n","        imgs = []\n","        for c, row in batch.iterrows():\n","            img = Image.fromarray(image_arrays[row['imgid']])\n","            imgs.append(img)\n","\n","        res = model(imgs, verbose=False)\n","\n","        batch_probs = []\n","        batch_cls = []\n","\n","        for r in res:\n","            pt = r.probs.data.cpu().numpy()\n","            batch_probs.append(pt)\n","            batch_cls.append(r.names[pt.argmax()])\n","\n","        probs.extend(batch_probs)\n","        cls_res.extend(batch_cls)\n","\n","    probs = np.array(probs)\n","    test['class'] = cls_res\n","    for k, v in res[0].names.items():\n","        test[v] = probs[:, k]\n","    return test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhxO640d7FMz","executionInfo":{"status":"ok","timestamp":1736717240394,"user_tz":300,"elapsed":7391770,"user":{"displayName":"xueer","userId":"12210123935039791816"}},"outputId":"b8169ee9-182d-4019-d8bd-dda10d5948ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["aug1 fold 0\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/1378 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 1378/1378 [02:12<00:00, 10.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(3316, 7) should be 3316\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/19373 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 19373/19373 [10:30<00:00, 30.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(19370, 7) (19373, 3)\n","aug1 fold 1\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/1378 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 1378/1378 [01:34<00:00, 14.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(3316, 7) should be 3316\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/19373 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 19373/19373 [10:27<00:00, 30.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(19370, 7) (19373, 3)\n","aug1 fold 2\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/1378 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 1378/1378 [01:40<00:00, 13.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(3316, 7) should be 3316\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/19373 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 19373/19373 [10:29<00:00, 30.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(19368, 7) (19373, 3)\n","aug1 fold 3\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/1378 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 1378/1378 [01:34<00:00, 14.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(3316, 7) should be 3316\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing images:   0%|          | 0/19373 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.49 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv9c summary (fused): 384 layers, 25,336,981 parameters, 0 gradients, 102.4 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 19373/19373 [10:29<00:00, 30.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["(19367, 7) (19373, 3)\n","crop fold 0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:06<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["crop fold 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:07<00:00,  3.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["crop fold 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:09<00:00,  3.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["crop fold 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:05<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate fold 0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:06<00:00,  3.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate fold 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:05<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate fold 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:05<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate fold 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:06<00:00,  3.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate50 fold 0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:05<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate50 fold 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:05<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate50 fold 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:06<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["rotate50 fold 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1211/1211 [06:06<00:00,  3.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 55min 7s, sys: 2min 9s, total: 1h 57min 17s\n","Wall time: 2h 3min 11s\n"]}],"source":["%%time\n","aug = True\n","fold = 0\n","subs = []\n","\n","test = pd.read_csv(f'{data_path}/test.csv')\n","test['bbox_count'] = test.groupby('NAME')['NAME'].transform('count')\n","test1 = test[test.bbox_count==1]\n","# img_dir = '{data_path}/images'\n","# print(img_dir)\n","# get predictions\n","\n","tids = test.groupby('NAME')['trustii_id'].agg(list).to_dict()\n","df = test.groupby('NAME').agg({'trustii_id':'count'})\n","df.columns = ['bbox_count']\n","df = df.reset_index()\n","assert df.shape[0] == len(test_imgs_array)\n","df['imgid'] = [f'arr_{c}' for c in range(len(df))]\n","mask = df.bbox_count == 1\n","df1 = df[mask]#[:100]\n","df2 = df[~mask]#[:100]\n","\n","# mt = test1.NAME.isin(df1.NAME) #| test1.NAME.isin(df2.NAME)\n","# test1 = test1[mt]\n","backup = []\n","for fold in range(4):\n","    det_model = YOLO(f'{data_path}/models/aug1/fold_{fold}_best.pt')\n","    print('aug1', f'fold {fold}')\n","    #sub2, probs2, names = predict_no_prob(df2, det_model, aug, test_imgs_array)\n","    sub2, probs2, names = predict_det(df2, det_model, aug, test_imgs_array, batch_size=1, max_det=11)\n","    print(sub2.shape, \"should be 3316\")\n","    sub1, probs1, names = predict_det(df1, det_model, aug, test_imgs_array, batch_size=32, max_det=1)\n","    print(sub1.shape, df1.shape)\n","    sub1.shape[0]+sub2.shape[0], test.shape[0]\n","\n","    df3 = df1[~df1.NAME.isin(sub1.NAME)]\n","    if df3.shape[0]:\n","        sub3 = sub1[:df3.shape[0]].copy()\n","        sub3['NAME'] = df3['NAME'].values\n","        sub3['trustii_id'] = sub3['NAME'].apply(lambda x: tids[x][0])\n","        probs3 = np.zeros((df3.shape[0], probs2.shape[1]))\n","        sub = pd.concat([sub1,sub2,sub3])\n","        probs = np.vstack([probs1, probs2, probs3])\n","    else:\n","        sub = pd.concat([sub1,sub2])\n","        probs = np.vstack([probs1, probs2])\n","\n","    for k,v in names.items():\n","        sub[v] = probs[:,k]\n","        # only keep 5 decimal places\n","        sub[v] = sub[v].round(5)\n","    sub = test[['trustii_id']].merge(sub, on='trustii_id', how='left')\n","    subs.append(sub)\n","    backup.append(sub.copy())\n","\n","df1 = df1.merge(subs[0],on='NAME',how='left')\n","for model_name in ['crop','rotate','rotate50']:\n","    for fold in range(4):\n","        cls_model = YOLO(f'{data_path}/models/{model_name}/fold_{fold}_best.pt')\n","        print(model_name, f'fold {fold}')\n","        sub = predict_cls(df1, cls_model, test_imgs_array, batch_size=16)\n","        sub = test1[['trustii_id','NAME']].merge(sub, on='NAME', how='left')\n","        subs.append(sub)\n","        backup.append(sub.copy())\n","\n","sub0 = subs[0]\n","class_names = sub0.columns[-23:]\n","sub0['bbox_count'] = sub0.groupby('NAME')['NAME'].transform('count')\n","mask = sub0.bbox_count == 1\n","probs = sub0.loc[mask, class_names].values\n","for sub1 in subs[1:]:\n","    sub1['bbox_count'] = sub1.groupby('NAME')['NAME'].transform('count')\n","    mask2 = sub1.bbox_count == 1\n","    probs += sub1.loc[mask2,class_names].values\n","sub0['class_id'] = 0\n","sub0.loc[mask, 'class_id'] = probs.argmax(axis=1)\n","sub0.loc[mask, 'class'] = sub0.loc[mask, 'class_id'].apply(lambda x: class_names[x])"]},{"cell_type":"code","source":["sub0 = sub0.reset_index(drop=True)\n","base = pd.read_csv(f'{data_path}/submissions/fix_test_aug1_crop_rotate_rotate50_avg.csv')\n","cols = [col for col in base.columns]\n","sub0[cols].to_csv(f'{data_path}/submissions/rerun_fix_test_aug1_crop_rotate_rotate50_avg.csv', index=False)"],"metadata":{"id":"r9OCvoDafCs6"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1tPdwVhFCnJJAZdAWC49BVoAgfv0FIPD0","authorship_tag":"ABX9TyPpMdjKfVzIhn2UAlt1D4DE"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}